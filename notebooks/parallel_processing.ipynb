{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6899a30-a1c7-458e-a0f8-bddb085d1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aicsimageio[all] --user\n",
    "!pip install bioformats_jar\n",
    "!pip install aicspylibczi>=3.1.1 fsspec>=2022.8.0\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa37a1-e220-4797-b2d0-339e9187d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicsimageio.writers import OmeTiffWriter\n",
    "from aicsimageio import AICSImage\n",
    "from skimage.io import imshow\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from aicspylibczi import CziFile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bf42d-ff6d-43d6-883a-388f63a7002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_czi_file(czi_file_path, output_folder, subpart_size):\n",
    "    df = pd.DataFrame(columns=('X', 'Y', 'Z', 'focus'))\n",
    "        \n",
    "    # Create a CziFile object\n",
    "    czi = CziFile(czi_file_path)\n",
    "\n",
    "    df = pd.DataFrame(columns=['X','Y','Z','focus'])\n",
    "    \n",
    "    # Get the shape of the data\n",
    "    size = czi.size\n",
    "    \n",
    "    print(czi.dims)\n",
    "    print(size)\n",
    "\n",
    "    # Check if the file is a mosaic\n",
    "    is_mosaic = czi.is_mosaic()\n",
    "\n",
    "    if not is_mosaic:\n",
    "        print(\"The provided CZI file is not a mosaic.\")\n",
    "        return\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    mosaic_data_shape = czi.read_mosaic(C=0, scale_factor=1, Z=0).shape\n",
    "    num_subparts_x = mosaic_data_shape[2] // subpart_size[0]\n",
    "    num_subparts_y = mosaic_data_shape[3] // subpart_size[1]\n",
    "    total_iterations = num_subparts_x * num_subparts_y * size[2]\n",
    "    \n",
    "    print(f\"Total integrations: X {num_subparts_x}, Y {num_subparts_y}, Z {size[2]}\")\n",
    "\n",
    "    progress_bar = tqdm(total=total_iterations, desc=f\"Processing {os.path.basename(czi_file_path)}\")\n",
    "\n",
    "    for z in range(size[2]):\n",
    "        mosaic_data = czi.read_mosaic(C=0, scale_factor=1, Z=z)\n",
    "\n",
    "        for x in range(num_subparts_x):\n",
    "            for y in range(num_subparts_y):\n",
    "                subpart = mosaic_data[:, :, x * subpart_size[0]:(x + 1) * subpart_size[0],\n",
    "                                      y * subpart_size[1]:(y + 1) * subpart_size[1], :]\n",
    "\n",
    "                aics_image = AICSImage(subpart)\n",
    "\n",
    "                output_file = os.path.join(output_folder, f\"subpart_z-{z}_x-{x}_y-{y}.ome.tiff\")\n",
    "                \n",
    "                focus = cv2.Laplacian(np.squeeze(subpart), cv2.CV_64F, ksize=5).var()\n",
    "                # Append Row to DataFrame\n",
    "                list_row = {\"X\": x, \"Y\": y, \"Z\": z, \"focus\": focus}\n",
    "                df.loc[len(df)]= list_row\n",
    "\n",
    "                OmeTiffWriter.save(cv2.cvtColor(np.squeeze(subpart), cv2.COLOR_BGR2RGB), output_file, dim_order=\"YXS\")\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    output_csv = os.path.join(output_folder, f\"_focus_estimate.csv\")\n",
    "    df.to_csv(output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2426a4c-4a72-4a66-8082-1a901514da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# czi_file_path = '../../230518_ML_pyl_OB/2023_05_21_Pyl-OB_001.czi'\n",
    "# name = \"2023_05_21_Pyl-OB_001\"\n",
    "\n",
    "# test\n",
    "# czi_file_path = '../../test/2023_05_22_Pyl-OB_010_small.czi'\n",
    "# name = \"output\"\n",
    "\n",
    "# Usage example:\n",
    "# czi_file_path = '../../230518_ML_pyl_OB/2023_05_22_Pyl-OB_024.czi'\n",
    "# output_folder = \"2023_05_22_Pyl-OB_024\"\n",
    "\n",
    "czi_file_path = '../../230518_ML_pyl_OB/2023_05_22_Pyl-OB_023.czi'\n",
    "output_folder = \"2023_05_22_Pyl-OB_023\"\n",
    "\n",
    "subpart_size = [1600, 1920]\n",
    "# process_czi_file(czi_file_path, output_folder,subpart_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67ee554-9453-482b-91a4-0888ce69604e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eecc2df-eaa0-4f2b-b6a4-65e5880ca33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker_function(czi_file_path, output_folder,subpart_size):\n",
    "    try:\n",
    "        print(czi_file_path)\n",
    "        print(output_folder)\n",
    "        print(subpart_size)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(\"Exception in worker_function:\", e)\n",
    "        return 0\n",
    "\n",
    "num_processes = 4  # Number of parallel processes\n",
    "\n",
    "# Create some data to be processed\n",
    "input_data1 = ['path1', 'path2', 'path3', 'path4', 'path1', 'path2', 'path3', 'path4']\n",
    "input_data2 = ['name1', 'name2', 'name3', 'name4', 'name1', 'name2', 'name3', 'name4']\n",
    "input_data3 = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Split the input data into chunks for each process\n",
    "chunk_size = len(input_data1) // num_processes\n",
    "data_chunks1 = [input_data1[i:i + chunk_size] for i in range(0, len(input_data1), chunk_size)]\n",
    "data_chunks2 = [input_data2[i:i + chunk_size] for i in range(0, len(input_data2), chunk_size)]\n",
    "data_chunks3 = [input_data3[i:i + chunk_size] for i in range(0, len(input_data3), chunk_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39f91b-0d1a-4745-b0d8-8061b126f036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a pool of 4 worker processes\n",
      "Distribute the data chunks to worker processes for parallel processing\n"
     ]
    }
   ],
   "source": [
    "# Create a pool of worker processes\n",
    "print(f\"Create a pool of {num_processes} worker processes\")\n",
    "pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "try:\n",
    "    # Distribute the data chunks to worker processes for parallel processing\n",
    "    print('Distribute the data chunks to worker processes for parallel processing')\n",
    "    pool.starmap(worker_function, zip(data_chunks1, data_chunks2, data_chunks3))\n",
    "except Exception as e:\n",
    "        print(\"Exception in worker_function:\", e)\n",
    "\n",
    "\n",
    "# Close the pool of worker processes\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Print the final result\n",
    "print(\"Final Result\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778e231-5789-464e-a2d4-2e870ec81027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# def worker_function(data1, data2, data3):\n",
    "#     result = []  # Store results for this process\n",
    "#     for item1, item2, item3 in zip(data1, data2, data3):\n",
    "#         # Perform some computation on the data\n",
    "#         result.append(item1 * item2 + item3)\n",
    "#     return result\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     num_processes = 4  # Number of parallel processes\n",
    "\n",
    "#     # Create some data to be processed\n",
    "#     input_data1 = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "#     input_data2 = [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "#     input_data3 = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "#     # Split the input data into chunks for each process\n",
    "#     chunk_size = len(input_data1) // num_processes\n",
    "#     data_chunks1 = [input_data1[i:i + chunk_size] for i in range(0, len(input_data1), chunk_size)]\n",
    "#     data_chunks2 = [input_data2[i:i + chunk_size] for i in range(0, len(input_data2), chunk_size)]\n",
    "#     data_chunks3 = [input_data3[i:i + chunk_size] for i in range(0, len(input_data3), chunk_size)]\n",
    "\n",
    "#     # Create a pool of worker processes\n",
    "#     pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "#     # Distribute the data chunks to worker processes for parallel processing\n",
    "#     results = pool.starmap(worker_function, zip(data_chunks1, data_chunks2, data_chunks3))\n",
    "\n",
    "#     # Combine the results from all processes\n",
    "#     final_result = []\n",
    "#     for result in results:\n",
    "#         final_result.extend(result)\n",
    "\n",
    "#     # Close the pool of worker processes\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     # Print the final result\n",
    "#     print(\"Final Result:\", final_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
